{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67cf6cc2-b3cf-4843-b502-71872d3dde35",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Retrieval-augmented Generation (RAG) Using LLMs \n",
    "\n",
    "In this notebook, we create a basic prototype of a retrieval-augmented generation system. This prototype demonstates how traditional information retrieval methods can be combined with LLM-based text summarization to search through large documents or collection of documents.\n",
    "\n",
    "We use a Google Pixel 7 release notes as the input document. It is available in the `tensor-house-data` repository."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c0804d-029e-416d-8014-c7828d96e4d9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Environment Setup and Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a97eb6a-ccec-4527-b9fb-36b69c6ff3de",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Initialize LLM provider\n",
    "# (google-cloud-aiplatform must be installed)\n",
    "#\n",
    "from google.cloud import aiplatform\n",
    "aiplatform.init(\n",
    "    project='<< specify your project name here >>',\n",
    "    location='us-central1'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3eb6e94-a2f2-4548-8d0c-f4934f0224f6",
   "metadata": {},
   "source": [
    "## Question Answering Over a Large Document\n",
    "\n",
    "In this section, we demonstrate how standalone questions can be answered. The input document(s) is split inot chunks which are then indexed in a vector store. To answer the user question, the most relevant chunks are retrieved and passed to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3857ccc4-2559-493d-9e44-babb744576db",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input document has been split into 2 chunks\n",
      "\n",
      "The three coolest Pixel 7 features are:\n",
      "\n",
      "* Next-generation Super Res Zoom up to 8x on Pixel 7 and up to 30x on Pixel 7 Pro.\n",
      "* Macro Focus, which delivers Pixel HDR+ photo quality from as close as three centimeters away.\n",
      "* Photo Unblur, a Google Photos feature only on Pixel 7 and Pixel 7 Pro. Photo Unblur uses machine learning to improve your blurry pictures â€“ even old ones.\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import VertexAI\n",
    "\n",
    "from langchain.chains import RetrievalQA, ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "from langchain.document_loaders import UnstructuredHTMLLoader\n",
    "from langchain.embeddings.vertexai import VertexAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "#\n",
    "# Load the input document\n",
    "#\n",
    "loader = UnstructuredHTMLLoader(\"../../tensor-house-data/search/news-feeds/pixel-7-release.html\")\n",
    "documents = loader.load()\n",
    "\n",
    "#\n",
    "# Splitting\n",
    "#\n",
    "text_splitter = CharacterTextSplitter(chunk_size=4000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "print(f'The input document has been split into {len(texts)} chunks\\n')\n",
    "\n",
    "#\n",
    "# Indexing and storing\n",
    "#\n",
    "embeddings = VertexAIEmbeddings()\n",
    "docsearch = Chroma.from_documents(texts, embeddings)\n",
    "\n",
    "#\n",
    "# Querying\n",
    "#\n",
    "llm = VertexAI(temperature=0.7, verbose=True)\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=docsearch.as_retriever(search_kwargs={\"k\": 2}), return_source_documents=True)\n",
    "\n",
    "question = \"What are the three coolest Pixel 7 features? Please provide a list with short summaries.\"\n",
    "response = qa_chain({\"query\": question})\n",
    "\n",
    "print(response['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1eb2aa-9c11-429d-94eb-a718d44a3979",
   "metadata": {},
   "source": [
    "## Conversational Retrieval\n",
    "\n",
    "In this section, we prototype a conversational retrieval system. It combines the chat history with the retrieved documents to answer the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "96697fd3-4efc-4e98-a61d-ae722d142289",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are the three coolest Pixel features? \n",
      "\n",
      "The three coolest Pixel features are:\n",
      "\n",
      "1. Super Res Zoom up to 8x on Pixel 7 and up to 30x on Pixel 7 Pro.\n",
      "2. Macro Focus, which delivers Pixel HDR+ photo quality from as close as three centimeters away.\n",
      "3. Photo Unblur, a Google Photos feature only on Pixel 7 and Pixel 7 Pro. \n",
      "\n",
      "Can you explain the first feature in more detail? \n",
      "\n",
      "Super Res Zoom is a camera feature that uses machine learning to improve the quality of zoomed-in images. It works by taking multiple photos at different focal lengths and then stitching them together to create a single, high-resolution image. This results in images that are sharper and have less noise than images taken with a traditional zoom lens. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Initialize new chat\n",
    "#\n",
    "llm = VertexAI(verbose=True)\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "chat = ConversationalRetrievalChain.from_llm(llm, retriever=docsearch.as_retriever(search_kwargs={\"k\": 2}), memory=memory, verbose=False, return_generated_question=False)\n",
    "\n",
    "#\n",
    "# Ask questions with continuous context\n",
    "#\n",
    "\n",
    "def print_last_chat_turn(chat_history):\n",
    "    print(chat_history[-2].content, '\\n')\n",
    "    print(chat_history[-1].content, '\\n')    \n",
    "\n",
    "result = chat({\"question\": \"What are the three coolest Pixel features?\"})\n",
    "print_last_chat_turn(result['chat_history'])\n",
    "\n",
    "result = chat({\"question\": \"Can you explain the first feature in more detail?\"})\n",
    "print_last_chat_turn(result['chat_history'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
